{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZLZPF0dnLo"
      },
      "source": [
        "# Google Gemini 2.0 with MCP (Model Context Protocol) Servers\n",
        "\n",
        "Gemini models can be used with MCP server using its native tool calling capabilities. MCP, or Model Context Protocol, is an open standard introduced by Anthropic designed to standardize how AI models like Gemini interact with external tools and data sources. Instead of requiring custom integrations for each tool, MCP provides a structured way for models to access context, such as functions (tools), data sources (resources), or pre-defined prompts. This allows AI agents to securely and efficiently connect with real-world systems and workflows.\n",
        "\n",
        "MCP server expose their tools via JSON schema definitions, which can be converted to Gemini compatible OpenAPI schema definitions. This allows you to easily use MCP server with Gemini models, below you will example on how to implement this.\n",
        "\n",
        "You can learn more about Google Search integration with Gemini here:\n",
        "- [https://ai.google.dev/gemini-api/docs/function-calling?lang=python](https://ai.google.dev/gemini-api/docs/function-calling?lang=python&example=weather)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R08lmeGPdnLq",
        "outputId": "6e621b89-688f-42cc-aa2e-6e0fd7e6be90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Collecting mcp\n",
            "  Downloading mcp-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.0)\n",
            "Collecting httpx-sse>=0.4 (from mcp)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting starlette>=0.27 (from mcp)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting uvicorn>=0.23.1 (from mcp)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.5.2->mcp)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.1->mcp) (8.1.8)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Downloading mcp-1.6.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: uvicorn, python-dotenv, httpx-sse, starlette, sse-starlette, pydantic-settings, mcp\n",
            "Successfully installed httpx-sse-0.4.0 mcp-1.6.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 sse-starlette-2.2.1 starlette-0.46.1 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "# install Google GenAI and MCP\n",
        "%pip install google-genai mcp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TutEcng_dnLr"
      },
      "source": [
        "## Simple Example on how to use MCP with Gemini's tool calling\n",
        "\n",
        "MCPs can be used with Google DeepMind Gemini by converting the MCP tools into Gemini compatible tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W86On0lBdnLs",
        "outputId": "126644c7-e67f-4102-b72d-9c7194977eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function to call: airbnb_search\n",
            "Arguments: {'checkout': '2024-03-30', 'checkin': '2024-03-28', 'location': 'Paris'}\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "import os\n",
        "\n",
        "client = genai.Client(\n",
        "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
        ")  # Replace with your actual API key setup\n",
        "\n",
        "\n",
        "# Create server parameters for stdio connection\n",
        "server_params = StdioServerParameters(\n",
        "    command=\"npx\",  # Executable\n",
        "    args=[\n",
        "        \"-y\",\n",
        "        \"@openbnb/mcp-server-airbnb\",\n",
        "        \"--ignore-robots-txt\",\n",
        "    ],  # Optional command line arguments\n",
        "    env=None,  # Optional environment variables\n",
        ")\n",
        "\n",
        "async def run():\n",
        "    async with stdio_client(server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
        "            # Initialize the connection\n",
        "            await session.initialize()\n",
        "\n",
        "            # Get tools from MCP session and convert to Gemini Tool objects\n",
        "            mcp_tools = await session.list_tools()\n",
        "            tools = types.Tool(function_declarations=[\n",
        "                {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.inputSchema,\n",
        "                }\n",
        "                for tool in mcp_tools.tools\n",
        "            ])\n",
        "\n",
        "            # Send request with function declarations\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",  # Or your preferred model supporting function calling\n",
        "                contents=prompt,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    temperature=0.7,\n",
        "                    tools=[tools],\n",
        "                ),  # Example other config\n",
        "            )\n",
        "        # Check for a function call\n",
        "        if response.candidates[0].content.parts[0].function_call:\n",
        "            function_call = response.candidates[0].content.parts[0].function_call\n",
        "            print(f\"Function to call: {function_call.name}\")\n",
        "            print(f\"Arguments: {function_call.args}\")\n",
        "            # In a real app, you would call your function here:\n",
        "            # result = await session.call_tool(function_call.args, arguments=function_call.args)\n",
        "            # sent new request with function call\n",
        "        else:\n",
        "            print(\"No function call found in the response.\")\n",
        "            print(response.text)\n",
        "\n",
        "await run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpPzA1ekdnLs"
      },
      "source": [
        "## Full Agentic example with Gemini and Airbnb MCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAZwyvL6dnLt",
        "outputId": "bdd99d93-9101-42ac-b342-9acb623d3580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running agent loop with prompt: I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\n",
            "Attempting to call MCP tool: 'airbnb_search' with args: {'checkout': '2024-03-30', 'checkin': '2024-03-28', 'location': 'Paris'}\n",
            "MCP tool 'airbnb_search' executed successfully.\n",
            "Added 1 tool response parts to history.\n",
            "Making subsequent API call with tool responses...\n",
            "MCP tool calling loop finished. Returning final response.\n",
            "OK. I have searched for apartments in Paris for 2 nights from 2024-03-28 to 2024-03-30. Here are some of the results:\n",
            "* Room in Paris: https://www.airbnb.com/rooms/1192814572435529819\n",
            "* Room with balcony very close to Paris: https://www.airbnb.com/rooms/1073307779802809893\n",
            "* Small charming two-room in the middle of the courtyard: https://www.airbnb.com/rooms/51646829\n",
            "* A room 30 minutes from Paris with the RER C. tram9: https://www.airbnb.com/rooms/16219327\n",
            "* Le Voyage Paris: https://www.airbnb.com/rooms/1236913077555360681\n",
            "* Small Functional Studio: https://www.airbnb.com/rooms/1314305750209848961\n",
            "* Lovely little duplex house: https://www.airbnb.com/rooms/3599795\n",
            "* Cozy & Bright Parisian Style - 2P - Metro 400m: https://www.airbnb.com/rooms/1338982550925585307\n",
            "* Charming studio in the heart of the 14th arrondissement in Paris: https://www.airbnb.com/rooms/1161505367060701505\n",
            "* Service room 9 m2 , Trocadéro: https://www.airbnb.com/rooms/11137901\n",
            "* Very quiet 13 m² studio: https://www.airbnb.com/rooms/1058613505664263800\n",
            "* Luxembourg Gardens Room-Suite: https://www.airbnb.com/rooms/1348422275502236977\n",
            "* Charming Marais/Bastille studio: https://www.airbnb.com/rooms/1121566618691941945\n",
            "* ☀️ Cosy studio close to the Buttes Chaumont: https://www.airbnb.com/rooms/25961524\n",
            "* Bedroom 1 in charming Parisian house: https://www.airbnb.com/rooms/10625291\n",
            "* Room with balcony at Mairie de Montreuil: https://www.airbnb.com/rooms/596094203335263543\n",
            "* A very bright room/balcony: https://www.airbnb.com/rooms/8436272\n",
            "* Nice room in 3 rooms in Ménilmontant 3 cats: https://www.airbnb.com/rooms/15532830\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "import os\n",
        "\n",
        "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "model = \"gemini-2.0-flash\"\n",
        "\n",
        "# Create server parameters for stdio connection\n",
        "server_params = StdioServerParameters(\n",
        "    command=\"npx\",  # Executable\n",
        "    args=[\n",
        "        \"-y\",\n",
        "        \"@openbnb/mcp-server-airbnb\",\n",
        "        \"--ignore-robots-txt\",\n",
        "    ],  # Optional command line arguments\n",
        "    env=None,  # Optional environment variables\n",
        ")\n",
        "\n",
        "async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):\n",
        "    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
        "    # Initialize the connection\n",
        "    await session.initialize()\n",
        "\n",
        "    # --- 1. Get Tools from Session and convert to Gemini Tool objects ---\n",
        "    mcp_tools = await session.list_tools()\n",
        "    tools = types.Tool(function_declarations=[\n",
        "        {\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description,\n",
        "            \"parameters\": tool.inputSchema,\n",
        "        }\n",
        "        for tool in mcp_tools.tools\n",
        "    ])\n",
        "\n",
        "    # --- 2. Initial Request with user prompt and function declarations ---\n",
        "    response = await client.aio.models.generate_content(\n",
        "        model=model,  # Or your preferred model supporting function calling\n",
        "        contents=contents,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0,\n",
        "            tools=[tools],\n",
        "        ),  # Example other config\n",
        "    )\n",
        "\n",
        "    # --- 3. Append initial response to contents ---\n",
        "    contents.append(response.candidates[0].content)\n",
        "\n",
        "    # --- 4. Tool Calling Loop ---\n",
        "    turn_count = 0\n",
        "    max_tool_turns = 5\n",
        "    while response.function_calls and turn_count < max_tool_turns:\n",
        "        turn_count += 1\n",
        "        tool_response_parts: List[types.Part] = []\n",
        "\n",
        "        # --- 4.1 Process all function calls in order and return in this turn ---\n",
        "        for fc_part in response.function_calls:\n",
        "            tool_name = fc_part.name\n",
        "            args = fc_part.args or {}  # Ensure args is a dict\n",
        "            print(f\"Attempting to call MCP tool: '{tool_name}' with args: {args}\")\n",
        "\n",
        "            tool_response: dict\n",
        "            try:\n",
        "                # Call the session's tool executor\n",
        "                tool_result = await session.call_tool(tool_name, args)\n",
        "                print(f\"MCP tool '{tool_name}' executed successfully.\")\n",
        "                if tool_result.isError:\n",
        "                    tool_response = {\"error\": tool_result.content[0].text}\n",
        "                else:\n",
        "                    tool_response = {\"result\": tool_result.content[0].text}\n",
        "            except Exception as e:\n",
        "                tool_response = {\"error\":  f\"Tool execution failed: {type(e).__name__}: {e}\"}\n",
        "\n",
        "            # Prepare FunctionResponse Part\n",
        "            tool_response_parts.append(\n",
        "                types.Part.from_function_response(\n",
        "                    name=tool_name, response=tool_response\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # --- 4.2 Add the tool response(s) to history ---\n",
        "        contents.append(types.Content(role=\"user\", parts=tool_response_parts))\n",
        "        print(f\"Added {len(tool_response_parts)} tool response parts to history.\")\n",
        "\n",
        "        # --- 4.3 Make the next call to the model with updated history ---\n",
        "        print(\"Making subsequent API call with tool responses...\")\n",
        "        response = await client.aio.models.generate_content(\n",
        "            model=model,\n",
        "            contents=contents,  # Send updated history\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=1.0,\n",
        "                tools=[tools],\n",
        "            ),  # Keep sending same config\n",
        "        )\n",
        "        contents.append(response.candidates[0].content)\n",
        "\n",
        "    if turn_count >= max_tool_turns and response.function_calls:\n",
        "        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop.\")\n",
        "\n",
        "    print(\"MCP tool calling loop finished. Returning final response.\")\n",
        "    # --- 5. Return Final Response ---\n",
        "    return response\n",
        "\n",
        "async def run():\n",
        "    async with stdio_client(server_params) as (read, write):\n",
        "        async with ClientSession(\n",
        "            read,\n",
        "            write,\n",
        "        ) as session:\n",
        "            # Test prompt\n",
        "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
        "            print(f\"Running agent loop with prompt: {prompt}\")\n",
        "            # Run agent loop\n",
        "            res = await agent_loop(prompt, client, session)\n",
        "            return res\n",
        "res = await run()\n",
        "print(res.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY62Lh-TdnLt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}